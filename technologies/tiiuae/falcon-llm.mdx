---
title: "Falcon"
author: "Technology Innovation Institute (TII)"
description: "Falcon models by TII are advanced large language models (LLMs) that excel in natural language processing (NLP). They support diverse applications from conversational AI to data analysis, using state-of-the-art architectures for high performance and accuracy."
---

# Falcon LLM Models

Falcon models, developed by the Technology Innovation Institute (TII), are a family of advanced large language models (LLMs) designed to push the boundaries of natural language processing (NLP). These models facilitate a wide range of applications, from conversational AI to complex data analysis, making them versatile tools for developers, researchers, and businesses. Falcon models leverage extensive datasets and cutting-edge architectures to deliver high performance and accuracy in understanding and generating human language.

| General     |                                                                  |
| ----------- | ---------------------------------------------------------------- |
| Relese date | June, 2023                                                       |
| Author      | [Technology Innovation Institute (TII)](https://www.tii.ae/)     |
| Website     | https://falconllm.tii.ae/falcon-models.html                      |
| Type        | Large Language Model                                             |

## Key Features

- **Multilingual Capabilities**: All Falcon models have been trained on diverse, multilingual datasets. This extensive training allows the models to understand and generate text across multiple languages, making them valuable tools for global applications.

- **Efficient Architecture**: The models incorporate advanced architectural techniques such as flash attention and multi-query attention. These innovations optimize the models' performance, reducing computational overhead while maintaining high-quality outputs.

- **Scalability**: The Falcon family offers unprecedented flexibility for various use cases and computational constraints. This scalability allows developers and researchers to choose the most appropriate model based on their specific requirements and available computational resources.

- **Open-source**: The entire Falcon family is open-source. This transparency not only fosters trust but also enables community-driven improvements and innovations, potentially accelerating the models' development and application in various fields.

- **Contextual Understanding**: Falcon models demonstrate a remarkable ability to understand context and nuance in language, allowing them to generate more coherent and contextually appropriate responses across a wide range of topics.


## Falcon Models Family

The **Falcon Models Family** is a collection of advanced large language models created by the Technology Innovation Institute (TII). This family includes various models optimized for different scales and applications, ranging from text generation to data analysis. Built on extensive datasets and sophisticated architectures, Falcon models deliver high performance and accuracy in natural language processing. The lineup includes models like Falcon-1B, designed for smaller tasks, up to Falcon-180B, which handles the most complex applications with ease. Each model in the family is engineered to be versatile, making it a valuable tool for enhancing a wide range of systems.

* **Falcon-1B**: a smaller model providing a balance between performance and resource usage, suitable for basic NLP tasks.

* **Falcon-7B**: offers enhanced capabilities over Falcon-1B, ideal for more complex applications.

* **Falcon-11B**: a larger model designed for advanced language understanding and generation.

* **Falcon-40B**: provides extensive power for very large-scale NLP tasks.

* **Falcon-180B**: the most powerful model, handling the most demanding language applications.

* **Falcon-RefinedWeb**: a dataset supporting the training and fine-tuning of Falcon models with refined web data.


## Falcon 2 Models Family

The **Falcon 2 Family** is focused on increased usability and integrability, building a multi-modal ecosystem. Released not only the base 11B LLM, but also the 11B VLM model that incorporates image understanding capabilities. The vision-language model, or VLM, will allow users to engage in chats about visual content using text.

* **Falcon 2 11B**: a more efficient and accessible LLM trained on 5.5 trillion tokens with 11 billion parameters.

* **Falcon 2 11B VLM**: distinguished by its vision-to-language model (VLM) capabilities.


Read more about Falcon LLM Family: 

ðŸ‘‰ (https://huggingface.co/docs/transformers/main/en/model_doc/falcon)

ðŸ‘‰ (https://huggingface.co/blog/falcon2-11b)


## Falcon LLM Tutorials

Explore the lablab.ai use cases and applications to see how Falcon models can enhance your projects:

* [Falcon LLMs: In-depth Tutorial](https://lablab.ai/t/falcon-llms-models-tutorial)

* [Comprehensive Guide to AI71's API Hub](https://lablab.ai/t/ai71-platform-guide)

* [Building an AI-Powered Personal Health Dashboard with Falcon 180B](https://lablab.ai/t/building-an-ai-powered-personal-health-dashboard-with-falcon-180b)


## Start building with Falcon Models

Falcon LLMs have been optimized for various tasks, including text generation, translation, summarization, and more. With a focus on refined web data, these models aim to provide high-quality outputs that can be easily integrated into existing systems. Whether you are developing chatbots, content generation tools, or sophisticated data analysis platforms, Falcon models offer robust solutions to meet your needs.

To use the Falcon model, you need to have the `transformers` library installed. You can install it using `pip`:

```bash
pip install transformers
```

You can then use the Falcon model in your Python code as follows:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-40b"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Girafatron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")
```

## Links

* [Falcon Models Overview](https://falconllm.tii.ae/falcon-models.html)

* [Falcon-1B](https://huggingface.co/tiiuae/falcon-rw-1b) 

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) 

* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)

* [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B)

* [Falcon-11B](https://huggingface.co/tiiuae/falcon-11B)

* [Falcon RefinedWeb Dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)

* [Falcon 2 11B](https://huggingface.co/blog/falcon2-11b)

* [Falcon 2 11B VLM](https://huggingface.co/blog/falcon2-11b)

## Boilerplates

**Basic Integration for Falcon-11B**: Use the following boilerplate code to integrate the Falcon-11B model into your application:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "tiiuae/falcon-11B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example usage
inputs = tokenizer("Hello, how can I help you?", return_tensors="pt")
outputs = model.generate(inputs.input_ids, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

**Basic Integration for Falcon-2-40B with Vision**: To integrate the Falcon-2-40B model with vision capabilities:

```python
from transformers import AutoModelForVision, AutoTokenizer

model_name = "tiiuae/falcon-2-40B"
model = AutoModelForVision.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example usage
inputs = tokenizer("Describe this image.", return_tensors="pt")
outputs = model.generate(inputs.input_ids, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

Falcon models represent a significant advancement in the field of NLP, offering developers and researchers robust tools to create intelligent and responsive applications. By leveraging the extensive documentation, libraries, and community resources, you can start building and innovating with Falcon models today.


